{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7fde0a1f-6c36-4aca-89f9-8b3ed681ce34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ml-agents already installed\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import mlagents\n",
    "    print(\"ml-agents already installed\")\n",
    "except ImportError:\n",
    "    !python -m pip install -q mlagents==0.27.0\n",
    "    print(\"Installed ml-agents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7be426-8896-4e81-9e1e-7a9919852e71",
   "metadata": {},
   "source": [
    "# Q-Learning Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5314e27-74d6-47d2-aaa1-909ac74b4bcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import datetime\n",
    "import traceback\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Dense, LayerNormalization\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.optimizers import Adam, SGD, RMSprop\n",
    "from tensorflow.keras.losses import Huber, LogCosh, CategoricalCrossentropy, MeanSquaredError\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "\n",
    "from mlagents_envs.environment import UnityEnvironment\n",
    "from mlagents_envs.environment import ActionTuple, BaseEnv\n",
    "\n",
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8468de0a-eef4-458f-8a60-d2e078f16299",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Deep Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9b0d7f8-9c37-4015-b1e1-05ea0e267a31",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Configuration paramaters for the whole setup\n",
    "gamma = 0.01  # Discount factor for past rewards\n",
    "epsilon = 0.5  # Epsilon greedy parameter\n",
    "epsilon_min = 0.1  # Minimum epsilon greedy parameter\n",
    "epsilon_max = 1.0  # Maximum epsilon greedy parameter\n",
    "epsilon_interval = (\n",
    "    epsilon_max - epsilon_min\n",
    ")  # Rate at which to reduce chance of random action being taken\n",
    "max_steps_per_episode = 10000\n",
    "\n",
    "behavior_name = 'AlphaRomeo?team=0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd2f05b0-3eb1-4411-ba10-8a203ff44474",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_inputs = 15\n",
    "num_actions = 8\n",
    "\n",
    "num_hidden = 512\n",
    "\n",
    "def create_q_model():\n",
    "    with tf.device('GPU:0'):\n",
    "        # Convolutions on the frames on the screen\n",
    "        inputs = Input(shape=(num_inputs,))\n",
    "        normal_inputs = LayerNormalization()\n",
    "        common = Dense(num_hidden, activation=\"linear\")(normal_inputs(inputs))\n",
    "#         common2 = Dense(num_hidden*3, activation=\"linear\")(common)\n",
    "#         common3 = Dense(num_hidden, activation=\"linear\")(common2)\n",
    "        action = Dense(num_actions, activation=\"linear\")(common)\n",
    "\n",
    "        return Model(inputs=inputs, outputs=action)\n",
    "\n",
    "\n",
    "# The first model makes the predictions for Q-values which are used to\n",
    "# make a action.\n",
    "model = create_q_model()\n",
    "# Build a target model for the prediction of future rewards.\n",
    "# The weights of a target model get updated every 10000 steps thus when the\n",
    "# loss between the Q-values is calculated the target Q-value is stable.\n",
    "model_target = create_q_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aef95d65-5f77-4381-bc73-d258eed240bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# In the Deepmind paper they use RMSProp however then Adam optimizer\n",
    "# improves training time\n",
    "optimizer = Adam(learning_rate=0.01, clipnorm=1.0)\n",
    "\n",
    "done=False\n",
    "# Experience replay buffers\n",
    "action_history = []\n",
    "state_history = []\n",
    "state_next_history = []\n",
    "rewards_history = []\n",
    "done_history = []\n",
    "episode_reward_history = []\n",
    "running_reward = 0\n",
    "episode_count = 0\n",
    "frame_count = 0\n",
    "# Number of frames for exploration\n",
    "epsilon_greedy_frames = 20\n",
    "# Maximum replay length\n",
    "# Note: The Deepmind paper suggests 1000000 however this causes memory issues\n",
    "max_memory_length = 50\n",
    "# Using huber loss for stability\n",
    "loss_function = Huber()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14cba493-61ca-42d4-bb4f-b87e574be47e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\carlo\\AppData\\Local\\Temp/ipykernel_31684/933838534.py\", line 96, in <module>\n",
      "    future_rewards = model_target.predict(np.array(state_next_sample))\n",
      "  File \"C:\\Users\\carlo\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1751, in predict\n",
      "    tmp_batch_outputs = self.predict_function(iterator)\n",
      "  File \"C:\\Users\\carlo\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\", line 885, in __call__\n",
      "    result = self._call(*args, **kwds)\n",
      "  File \"C:\\Users\\carlo\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\", line 924, in _call\n",
      "    results = self._stateful_fn(*args, **kwds)\n",
      "  File \"C:\\Users\\carlo\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 3039, in __call__\n",
      "    return graph_function._call_flat(\n",
      "  File \"C:\\Users\\carlo\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 1963, in _call_flat\n",
      "    return self._build_call_outputs(self._inference_function.call(\n",
      "  File \"C:\\Users\\carlo\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 591, in call\n",
      "    outputs = execute.execute(\n",
      "  File \"C:\\Users\\carlo\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\", line 59, in quick_execute\n",
      "    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name=\"Alpha Romeo\", seed=1, side_channels=[])\n",
    "log_folder = 'logs/{}'.format(datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "writer = tf.summary.create_file_writer(log_folder)\n",
    "\n",
    "# tf.debugging.experimental.enable_dump_debug_info(log_folder, tensor_debug_mode=\"FULL_HEALTH\", circular_buffer_size=-1)\n",
    "\n",
    "try:\n",
    "    while True:  # Run until solvedloss_function\n",
    "        with writer.as_default():\n",
    "            env.reset()\n",
    "\n",
    "            decision_steps, terminal_steps = env.get_steps(behavior_name)\n",
    "            if decision_steps:\n",
    "                state = decision_steps[0].obs[0]\n",
    "            if terminal_steps:\n",
    "                done = True                \n",
    "                state = terminal_steps[0].obs[0]\n",
    "\n",
    "            episode_reward = 0\n",
    "\n",
    "            for timestep in range(1, max_steps_per_episode):\n",
    "                # env.render(); Adding this line would show the attempts\n",
    "                # of the agent in a pop up window.\n",
    "                frame_count += 1\n",
    "\n",
    "                # Use epsilon-greedy for exploration\n",
    "                if  epsilon > np.random.rand(1)[0]:\n",
    "                    # Take random action\n",
    "                    action = random.choice([1,2,3,4,5,6,7,8])\n",
    "                    \n",
    "                else:\n",
    "                    # Predict action Q-values\n",
    "                    # From environment state\n",
    "                    state_tensor = tf.convert_to_tensor(state)\n",
    "                    state_tensor = tf.expand_dims(state_tensor, 0)\n",
    "                    action_probs = model(state_tensor, training=False)\n",
    "                    # Take best action\n",
    "                    action = action_probs[0]\n",
    "                    action = int(tf.argmax(action)+1)\n",
    "                    \n",
    "\n",
    "                # Decay probability of taking random action\n",
    "                epsilon -= epsilon_interval / epsilon_greedy_frames\n",
    "                epsilon = max(epsilon, epsilon_min)\n",
    "\n",
    "                # Apply the sampled action in our environment\n",
    "        #         state_next, reward, done, _ = env.step(action)\n",
    "                action_tuple = ActionTuple()\n",
    "                action_tuple.add_discrete(np.array([[action]]))\n",
    "                env.set_actions(behavior_name, action_tuple)\n",
    "                # Perform a step in the simulation\n",
    "                env.step()\n",
    "\n",
    "                decision_steps, terminal_steps = env.get_steps(behavior_name)\n",
    "\n",
    "                if terminal_steps:\n",
    "                    done = True\n",
    "                    env.reset()\n",
    "                    decision_steps, terminal_steps = env.get_steps(behavior_name)\n",
    "                    state_next = decision_steps[0].obs[0]\n",
    "    #                 decision_steps, terminal_steps = env.get_steps(behavior_name)\n",
    "                    break\n",
    "\n",
    "                else:\n",
    "                    state_next = decision_steps[0].obs[0]\n",
    "\n",
    "\n",
    "                reward = decision_steps[0].reward\n",
    "        #         state_next = np.array(state_next)\n",
    "\n",
    "                episode_reward += reward\n",
    "                # Save actions and states in replay buffer\n",
    "                action_history.append(action)\n",
    "                state_history.append(state)\n",
    "                state_next_history.append(state_next)\n",
    "                done_history.append(done) \n",
    "                rewards_history.append(reward)\n",
    "                state = state_next\n",
    "\n",
    "                # Update every fourth frame and once batch size is over 32\n",
    "\n",
    "\n",
    "                # Get indices of samples for replay buffers\n",
    "                #indice = np.random.choice(range(len(done_history)))\n",
    "\n",
    "                state_sample = state_history[0]\n",
    "                state_next_sample = np.array(state_next_history)\n",
    "                \n",
    "                rewards_sample = rewards_history\n",
    "                action_sample = action_history\n",
    "                done_sample = tf.convert_to_tensor([float(done) for done in done_history])\n",
    "\n",
    "                # Build the updated Q-values for the sampled future states\n",
    "                # Use the target model for stability\n",
    "                state_next_sample = tf.reshape(state_next_sample, [-1, num_inputs])\n",
    "                future_rewards = model_target.predict(np.array(state_next_sample))\n",
    "                \n",
    "                # Q value = reward + discount factor * expected future reward\n",
    "                updated_q_values = rewards_sample + gamma * tf.reduce_max(\n",
    "                    future_rewards, axis=1\n",
    "                )\n",
    "\n",
    "                # If final frame set the last value to -1\n",
    "                updated_q_values = updated_q_values * (1 - done_sample) - done_sample\n",
    "\n",
    "                # Create a mask so we only calculate loss on the updated Q-values\n",
    "\n",
    "                masks = tf.one_hot(action_sample, num_actions)\n",
    "\n",
    "                with tf.GradientTape() as tape:\n",
    "                    # Train the model on the states and updated Q-values\n",
    "                    q_values = model(np.array([state_sample]))\n",
    "                    \n",
    "\n",
    "                    # Apply the masks to the Q-values to get the Q-value for action taken\n",
    "                    q_action = tf.reduce_sum(tf.multiply(q_values, masks), axis=1)\n",
    "\n",
    "                    # Calculate loss between new Q-value and old Q-value\n",
    "                    \n",
    "                    loss = loss_function(updated_q_values, q_action)\n",
    "\n",
    "                # Backpropagation\n",
    "                grads = tape.gradient(loss, model.trainable_variables)\n",
    "                optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "\n",
    "            # update the the target network with new weights\n",
    "            model_target.set_weights(model.get_weights())\n",
    "            # Log details\n",
    "            template = \"running reward: {:.2f} at episode {}, frame count {}\"\n",
    "    #         print(template.format(running_reward, episode_count, frame_count))\n",
    "            tf.summary.scalar(name=\"reward\", data=running_reward, step=episode_count)\n",
    "            dqn_variable = model.trainable_variables\n",
    "            tf.summary.histogram(name=\"dqn_variables\", data=tf.convert_to_tensor(dqn_variable[0]), step=episode_count)\n",
    "            writer.flush()\n",
    "            # Limit the state and reward history\n",
    "            if len(rewards_history) > max_memory_length:\n",
    "                del rewards_history[:1]\n",
    "                del state_history[:1]\n",
    "                del state_next_history[:1]\n",
    "                del action_history[:1]\n",
    "                del done_history[:1]\n",
    "\n",
    "            if done:\n",
    "                env.reset()\n",
    "                done = False\n",
    "\n",
    "\n",
    "            # Update running reward to check condition for solving\n",
    "            episode_reward_history.append(episode_reward)\n",
    "            if len(episode_reward_history) > 100:\n",
    "                del episode_reward_history[:1]\n",
    "            running_reward = np.mean(episode_reward_history)\n",
    "\n",
    "            episode_count += 10\n",
    "\n",
    "            if running_reward > 500000:  # Condition to consider the task solved\n",
    "                print(\"Solved at episode {}!\".format(episode_count))\n",
    "                break\n",
    "except:\n",
    "    print(traceback.print_exc())\n",
    "    writer.close()\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "4c9c430e-ee6c-4189-800f-f75d4767affd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'kill' nÆo ‚ reconhecido como um comando interno\n",
      "ou externo, um programa oper vel ou um arquivo em lotes.\n"
     ]
    }
   ],
   "source": [
    "!kill 26364"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "b19fab9e-d030-44c4-a1c7-18fbe818e4aa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "env.set_actions('AlphaRomeo?team=0', action_tuple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "011ea144-df38-4dd6-9e2d-f72948da09d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "03ef27cf-a04d-4761-b431-a5e3bd703f57",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnityEnvironmentException",
     "evalue": "No Unity environment is loaded.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnityEnvironmentException\u001b[0m                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-36-1baceacf4cb1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\mlagents_envs\\environment.py\u001b[0m in \u001b[0;36mclose\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    410\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_close\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    411\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 412\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mUnityEnvironmentException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"No Unity environment is loaded.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    413\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    414\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_close\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnityEnvironmentException\u001b[0m: No Unity environment is loaded."
     ]
    }
   ],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887f76a6-4fb8-4396-9bea-f51b4b4ddce6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
